"""
FraudGuard AI - AnÃ¡lise ExploratÃ³ria de Dados
AnÃ¡lise completa do dataset de fraudes bancÃ¡rias
"""

import matplotlib
matplotlib.use('Agg')  # Backend sem interface grÃ¡fica

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

print("="*80)
print(" "*20 + "FRAUDGUARD AI - ANÃLISE EXPLORATÃ“RIA")
print("="*80)

# ============================================
# 1. CARREGAR DADOS
# ============================================
print("\nğŸ“‚ 1. CARREGANDO DADOS...")
print("-"*80)

try:
    df = pd.read_csv('datasets/fraud/creditcard.csv')
    print(f"âœ… Dataset carregado com sucesso!")
    print(f"   - Total de transaÃ§Ãµes: {len(df):,}")
    print(f"   - Total de features: {len(df.columns)}")
except Exception as e:
    print(f"âŒ Erro ao carregar dados: {e}")
    exit(1)

# ============================================
# 2. INFORMAÃ‡Ã•ES GERAIS
# ============================================
print("\nğŸ“Š 2. INFORMAÃ‡Ã•ES GERAIS DO DATASET")
print("-"*80)

print("\nğŸ”¹ Primeiras 5 linhas:")
print(df.head())

print("\nğŸ”¹ InformaÃ§Ãµes das colunas:")
print(df.info())

print("\nğŸ”¹ EstatÃ­sticas descritivas:")
print(df.describe())

print("\nğŸ”¹ Valores nulos:")
print(df.isnull().sum())

# ============================================
# 3. ANÃLISE DE FRAUDES
# ============================================
print("\nğŸ¯ 3. ANÃLISE DE FRAUDES")
print("-"*80)

fraud_count = df['Class'].value_counts()
fraud_percentage = df['Class'].value_counts(normalize=True) * 100

print(f"\nğŸ”¹ DistribuiÃ§Ã£o de Classes:")
print(f"   LegÃ­timas (0): {fraud_count[0]:,} ({fraud_percentage[0]:.2f}%)")
print(f"   Fraudes (1):   {fraud_count[1]:,} ({fraud_percentage[1]:.2f}%)")
print(f"\n   âš ï¸  Dataset ALTAMENTE DESBALANCEADO!")
print(f"   RazÃ£o: 1 fraude para cada {fraud_count[0]//fraud_count[1]} transaÃ§Ãµes legÃ­timas")

# GrÃ¡fico de pizza
plt.figure(figsize=(10, 5))

plt.subplot(1, 2, 1)
colors = ['#2ecc71', '#e74c3c']
explode = (0, 0.1)
plt.pie(fraud_count, labels=['LegÃ­tima', 'Fraude'], autopct='%1.2f%%', 
        colors=colors, explode=explode, shadow=True, startangle=90)
plt.title('DistribuiÃ§Ã£o de Classes', fontsize=14, fontweight='bold')

plt.subplot(1, 2, 2)
sns.countplot(data=df, x='Class', palette=colors)
plt.title('Contagem de TransaÃ§Ãµes', fontsize=14, fontweight='bold')
plt.xlabel('Classe (0=LegÃ­tima, 1=Fraude)')
plt.ylabel('Quantidade')
plt.xticks([0, 1], ['LegÃ­tima', 'Fraude'])

plt.tight_layout()
plt.savefig('datasets/fraud/01_class_distribution.png', dpi=300, bbox_inches='tight')
print("\nğŸ’¾ GrÃ¡fico salvo: datasets/fraud/01_class_distribution.png")
plt.close()

# ============================================
# 4. ANÃLISE DO VALOR DAS TRANSAÃ‡Ã•ES
# ============================================
print("\nğŸ’° 4. ANÃLISE DO VALOR DAS TRANSAÃ‡Ã•ES")
print("-"*80)

print("\nğŸ”¹ EstatÃ­sticas do valor (Amount):")
print(df['Amount'].describe())

print(f"\nğŸ”¹ ComparaÃ§Ã£o LegÃ­timas vs Fraudes:")
print(f"   Valor mÃ©dio - LegÃ­timas: ${df[df['Class']==0]['Amount'].mean():.2f}")
print(f"   Valor mÃ©dio - Fraudes:   ${df[df['Class']==1]['Amount'].mean():.2f}")
print(f"   Valor mÃ¡ximo - LegÃ­timas: ${df[df['Class']==0]['Amount'].max():.2f}")
print(f"   Valor mÃ¡ximo - Fraudes:   ${df[df['Class']==1]['Amount'].max():.2f}")

# GrÃ¡ficos de distribuiÃ§Ã£o de valores
fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# Histograma geral
axes[0, 0].hist(df['Amount'], bins=50, color='skyblue', edgecolor='black')
axes[0, 0].set_title('DistribuiÃ§Ã£o de Valores - Todas as TransaÃ§Ãµes', fontweight='bold')
axes[0, 0].set_xlabel('Valor ($)')
axes[0, 0].set_ylabel('FrequÃªncia')
axes[0, 0].set_yscale('log')

# Boxplot por classe
df.boxplot(column='Amount', by='Class', ax=axes[0, 1])
axes[0, 1].set_title('DistribuiÃ§Ã£o de Valores por Classe', fontweight='bold')
axes[0, 1].set_xlabel('Classe (0=LegÃ­tima, 1=Fraude)')
axes[0, 1].set_ylabel('Valor ($)')
plt.sca(axes[0, 1])
plt.xticks([1, 2], ['LegÃ­tima', 'Fraude'])

# Histograma legÃ­timas
axes[1, 0].hist(df[df['Class']==0]['Amount'], bins=50, color='#2ecc71', 
                edgecolor='black', alpha=0.7)
axes[1, 0].set_title('DistribuiÃ§Ã£o - TransaÃ§Ãµes LegÃ­timas', fontweight='bold')
axes[1, 0].set_xlabel('Valor ($)')
axes[1, 0].set_ylabel('FrequÃªncia')
axes[1, 0].set_yscale('log')

# Histograma fraudes
axes[1, 1].hist(df[df['Class']==1]['Amount'], bins=50, color='#e74c3c', 
                edgecolor='black', alpha=0.7)
axes[1, 1].set_title('DistribuiÃ§Ã£o - Fraudes', fontweight='bold')
axes[1, 1].set_xlabel('Valor ($)')
axes[1, 1].set_ylabel('FrequÃªncia')

plt.tight_layout()
plt.savefig('datasets/fraud/02_amount_distribution.png', dpi=300, bbox_inches='tight')
print("ğŸ’¾ GrÃ¡fico salvo: datasets/fraud/02_amount_distribution.png")
plt.close()

# ============================================
# 5. ANÃLISE TEMPORAL
# ============================================
print("\nâ° 5. ANÃLISE TEMPORAL")
print("-"*80)

print("\nğŸ”¹ EstatÃ­sticas de Tempo (Time):")
print(df['Time'].describe())

# Converter tempo em horas
df['Hour'] = (df['Time'] / 3600) % 24

print(f"\nğŸ”¹ TransaÃ§Ãµes por perÃ­odo:")
fraud_by_hour = df.groupby('Hour')['Class'].agg(['sum', 'count'])
fraud_by_hour.columns = ['Fraudes', 'Total']
fraud_by_hour['Taxa'] = (fraud_by_hour['Fraudes'] / fraud_by_hour['Total'] * 100)

print(fraud_by_hour)

# GrÃ¡fico temporal
fig, axes = plt.subplots(2, 1, figsize=(15, 10))

# TransaÃ§Ãµes ao longo do tempo
axes[0].plot(df[df['Class']==0]['Time'], df[df['Class']==0]['Amount'], 
             'g.', alpha=0.1, label='LegÃ­tima', markersize=1)
axes[0].plot(df[df['Class']==1]['Time'], df[df['Class']==1]['Amount'], 
             'r.', alpha=0.5, label='Fraude', markersize=3)
axes[0].set_title('TransaÃ§Ãµes ao Longo do Tempo', fontweight='bold', fontsize=14)
axes[0].set_xlabel('Tempo (segundos)')
axes[0].set_ylabel('Valor ($)')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# Fraudes por hora
hour_counts = df.groupby(['Hour', 'Class']).size().unstack(fill_value=0)
hour_counts.plot(kind='bar', ax=axes[1], color=['#2ecc71', '#e74c3c'], width=0.8)
axes[1].set_title('DistribuiÃ§Ã£o de TransaÃ§Ãµes por Hora', fontweight='bold', fontsize=14)
axes[1].set_xlabel('Hora do Dia')
axes[1].set_ylabel('NÃºmero de TransaÃ§Ãµes')
axes[1].legend(['LegÃ­tima', 'Fraude'])
axes[1].grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.savefig('datasets/fraud/03_temporal_analysis.png', dpi=300, bbox_inches='tight')
print("ğŸ’¾ GrÃ¡fico salvo: datasets/fraud/03_temporal_analysis.png")
plt.close()

# ============================================
# 6. ANÃLISE DAS FEATURES V1-V28
# ============================================
print("\nğŸ”¬ 6. ANÃLISE DAS FEATURES V1-V28 (PCA)")
print("-"*80)

# Selecionar apenas as features V
v_features = [col for col in df.columns if col.startswith('V')]

print(f"\nğŸ”¹ Total de features PCA: {len(v_features)}")

# CorrelaÃ§Ã£o mÃ©dia com a classe
correlations = df[v_features + ['Class']].corr()['Class'].drop('Class').abs().sort_values(ascending=False)

print(f"\nğŸ”¹ Top 10 features mais correlacionadas com Fraude:")
for i, (feature, corr) in enumerate(correlations.head(10).items(), 1):
    print(f"   {i:2d}. {feature}: {corr:.4f}")

# Heatmap das correlaÃ§Ãµes
plt.figure(figsize=(12, 10))
correlation_matrix = df[v_features[:14]].corr()  # Primeiras 14 features
sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', center=0, 
            square=True, linewidths=0.5)
plt.title('Mapa de CorrelaÃ§Ã£o - Features V1-V14', fontweight='bold', fontsize=14)
plt.tight_layout()
plt.savefig('datasets/fraud/04_correlation_heatmap.png', dpi=300, bbox_inches='tight')
print("\nğŸ’¾ GrÃ¡fico salvo: datasets/fraud/04_correlation_heatmap.png")
plt.close()

# DistribuiÃ§Ã£o das top features
fig, axes = plt.subplots(2, 3, figsize=(15, 10))
axes = axes.ravel()

for i, feature in enumerate(correlations.head(6).index):
    # LegÃ­timas
    axes[i].hist(df[df['Class']==0][feature], bins=50, alpha=0.5, 
                 color='#2ecc71', label='LegÃ­tima', density=True)
    # Fraudes
    axes[i].hist(df[df['Class']==1][feature], bins=50, alpha=0.5, 
                 color='#e74c3c', label='Fraude', density=True)
    axes[i].set_title(f'{feature} (corr: {correlations[feature]:.3f})', fontweight='bold')
    axes[i].legend()
    axes[i].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('datasets/fraud/05_top_features.png', dpi=300, bbox_inches='tight')
print("ğŸ’¾ GrÃ¡fico salvo: datasets/fraud/05_top_features.png")
plt.close()

# ============================================
# 7. PREPARAÃ‡ÃƒO DOS DADOS PARA MODELAGEM
# ============================================
print("\nğŸ”§ 7. PREPARAÃ‡ÃƒO DOS DADOS")
print("-"*80)

# Normalizar Amount e Time
scaler = StandardScaler()
df['Amount_scaled'] = scaler.fit_transform(df['Amount'].values.reshape(-1, 1))
df['Time_scaled'] = scaler.fit_transform(df['Time'].values.reshape(-1, 1))

print("âœ… Features 'Amount' e 'Time' normalizadas")

# Separar features e target
X = df.drop(['Class', 'Amount', 'Time', 'Hour'], axis=1)
y = df['Class']

print(f"\nğŸ”¹ Shape final dos dados:")
print(f"   Features (X): {X.shape}")
print(f"   Target (y): {y.shape}")

# Salvar dados processados
df.to_csv('datasets/fraud/creditcard_processed.csv', index=False)
print("\nğŸ’¾ Dados processados salvos: datasets/fraud/creditcard_processed.csv")

# ============================================
# 8. RESUMO ESTATÃSTICO
# ============================================
print("\nğŸ“ˆ 8. RESUMO ESTATÃSTICO FINAL")
print("-"*80)

print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    RESUMO DO DATASET                          â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ Total de transaÃ§Ãµes:        {len(df):>10,}                      â•‘
â•‘ TransaÃ§Ãµes legÃ­timas:       {fraud_count[0]:>10,} ({fraud_percentage[0]:>5.2f}%)      â•‘
â•‘ TransaÃ§Ãµes fraudulentas:    {fraud_count[1]:>10,} ({fraud_percentage[1]:>5.2f}%)       â•‘
â•‘                                                              â•‘
â•‘ Valor mÃ©dio - LegÃ­timas:    ${df[df['Class']==0]['Amount'].mean():>10,.2f}              â•‘
â•‘ Valor mÃ©dio - Fraudes:      ${df[df['Class']==1]['Amount'].mean():>10,.2f}              â•‘
â•‘                                                              â•‘
â•‘ Features totais:            {len(X.columns):>10}                          â•‘
â•‘ Features PCA (V1-V28):      {len(v_features):>10}                          â•‘
â•‘                                                              â•‘
â•‘ Top 3 features correlacionadas:                             â•‘
â•‘   1. {correlations.index[0]:>10}: {correlations.iloc[0]:>6.4f}                          â•‘
â•‘   2. {correlations.index[1]:>10}: {correlations.iloc[1]:>6.4f}                          â•‘
â•‘   3. {correlations.index[2]:>10}: {correlations.iloc[2]:>6.4f}                          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")

# ============================================
# 9. CONCLUSÃ•ES E PRÃ“XIMOS PASSOS
# ============================================
print("\nğŸ’¡ 9. CONCLUSÃ•ES E INSIGHTS")
print("-"*80)

print("""
ğŸ” INSIGHTS PRINCIPAIS:

1. DESBALANCEAMENTO EXTREMO:
   âš ï¸  Apenas 0.17% das transaÃ§Ãµes sÃ£o fraudes
   â†’ Precisaremos usar tÃ©cnicas de balanceamento (SMOTE, undersampling)
   â†’ MÃ©tricas como Precision, Recall e F1-Score sÃ£o mais importantes que Accuracy

2. CARACTERÃSTICAS DAS FRAUDES:
   ğŸ’° Fraudes tendem a ter valores mÃ©dios MENORES que transaÃ§Ãµes legÃ­timas
   â° PadrÃµes temporais podem indicar horÃ¡rios de maior risco
   
3. FEATURES MAIS RELEVANTES:
   ğŸ“Š V14, V4, V11, V12 e V10 sÃ£o as mais correlacionadas com fraude
   â†’ Essas features serÃ£o crÃ­ticas para nossos modelos

4. NORMALIZAÃ‡ÃƒO NECESSÃRIA:
   ğŸ”§ Amount e Time possuem escalas muito diferentes das features V
   â†’ JÃ¡ normalizamos essas variÃ¡veis para melhorar o desempenho dos modelos

ğŸ¯ PRÃ“XIMOS PASSOS:

   âœ… AnÃ¡lise ExploratÃ³ria: CONCLUÃDA
   â†’ Modelo de ClassificaÃ§Ã£o (Random Forest, XGBoost, SVM)
   â†’ Modelo de RegressÃ£o (Score de Risco)
   â†’ Modelo de Clustering (PadrÃµes de fraude)
   â†’ Modelo de VisÃ£o Computacional (OCR de dÃ­gitos)
   â†’ IntegraÃ§Ã£o e Deploy Web
""")

print("\n" + "="*80)
print(" "*25 + "ANÃLISE CONCLUÃDA COM SUCESSO! âœ…")
print("="*80)
print("\nğŸ“‚ Arquivos gerados:")
print("   â€¢ datasets/fraud/creditcard_processed.csv")
print("   â€¢ datasets/fraud/01_class_distribution.png")
print("   â€¢ datasets/fraud/02_amount_distribution.png")
print("   â€¢ datasets/fraud/03_temporal_analysis.png")
print("   â€¢ datasets/fraud/04_correlation_heatmap.png")
print("   â€¢ datasets/fraud/05_top_features.png")
print("\nğŸš€ Execute agora: python src/02_classification_model.py")
print("="*80)